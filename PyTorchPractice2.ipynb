{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchPractice2.ipynb",
      "provenance": [],
      "mount_file_id": "1yDLDyb52mf5gE5uRz5F2U-NPsGRUSjlq",
      "authorship_tag": "ABX9TyNHSb5NHaeYMb32Wb5k98dC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikhilKanamarla/ML-Practice/blob/master/PyTorchPractice2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-bcLqbe1gn1"
      },
      "source": [
        "Intro to the torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TobcNcQ6ok8J",
        "outputId": "f90e5afe-c188-4ccf-e8c8-52a17073405e"
      },
      "source": [
        "import torch\n",
        "#run terminal commands\n",
        "!nvidia-smi\n",
        "x = torch.empty(1)\n",
        "z = torch.zeros(2,3)\n",
        "print(x)\n",
        "print(z)\n",
        "#an array of data which runs well on the GPU\n",
        "x = torch.Tensor([5,3])\n",
        "print (x * x)\n",
        "y = torch.zeros([2,5])\n",
        "print(y)\n",
        "#check size of tensor\n",
        "print(y.shape)\n",
        "#random initalization\n",
        "y = torch.rand(2,5)\n",
        "print(y)\n",
        "#reshape tensor to fit input shape of neural network\n",
        "y = y.view([1,10])\n",
        "print(y.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 24 17:35:26 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "tensor([9.9591e-36])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([25.,  9.])\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "torch.Size([2, 5])\n",
            "tensor([[0.9835, 0.6031, 0.5135, 0.8401, 0.2756],\n",
            "        [0.5250, 0.3820, 0.7220, 0.5554, 0.0044]])\n",
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqr9f3q1bBg"
      },
      "source": [
        "Data in PyTorch Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ww87BeZO3B56",
        "outputId": "dbc05e8d-a6bc-4786-e9c2-405a0c7204c9"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "#get datasets using torchvision.datasets transforms are application applied to data (transforms conversion to tensors)\n",
        "train = torchvision.datasets.MNIST(\"\", train=True, download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test = torchvision.datasets.MNIST(\"\", train=False, download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
        "#store in data loader, batch size is how many samples is passed through the model at once (in GPU memory), best batch size is between 8-64\n",
        "#shuffling avoids feeding too much of one kind of image and leads to more generalization \n",
        "trainset = torch.utils.data.DataLoader(train,batch_size=10,shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test,batch_size=10,shuffle=True)\n",
        "#iterating over dataset, prints the batch of size 10 of tensor of tensors for images (image of digit) and then a tensor of tensors output of labels (digits) \n",
        "for data in trainset:\n",
        "  print(data)\n",
        "  break\n",
        "#print an individual image and visualize \n",
        "x,y = data[0][0], data[1][0]\n",
        "import matplotlib.pyplot as plt\n",
        "#default shape doesn't work for matplotlib so we need to resize\n",
        "print(x.shape)\n",
        "plt.imshow(x.view(28,28))\n",
        "#A dataset should be balanced such that every type of image is equally distributed and it avoid the model from cheating or overfitting \n",
        "total = 0\n",
        "counter_imageType = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
        "for data in trainset:\n",
        "  x, y = data\n",
        "  for y1 in y:\n",
        "    counter_imageType[int(y1)] +=1\n",
        "#gives distribution of images \n",
        "print(\"distribution of images\", counter_imageType)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 3, 7, 8, 3, 4, 6, 7, 6, 0])]\n",
            "torch.Size([1, 28, 28])\n",
            "distribution of images {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QqXqRFLsmGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ed55a6-127b-406a-b665-aac75789425b"
      },
      "source": [
        "#Building a machine learning model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "#neural network class\n",
        "class Net(nn.Module):\n",
        "  #intialize class\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #feedforward neural network passes data from input layer to output layer\n",
        "    #fully connected layer with input shape of 28*28 pixels (flatten image to one row) and output feature is size 64. Linear means flat layer\n",
        "    self.fc1 = nn.Linear(28*28,64)\n",
        "    #feed in data from fc1 to fc2\n",
        "    self.fc2 = nn.Linear(64,64)\n",
        "    self.fc3 = nn.Linear(64,64)\n",
        "    #output layer has input 64 and output of size 10 to represent 10 classes in MNIST\n",
        "    self.fc4 = nn.Linear(64,10)\n",
        "  #forward pass through the data \n",
        "  def forward(self, x):\n",
        "    #relu is activation function and performs operation on input data\n",
        "    #input and output dimenson of relu are the same\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    #softmax function gets probability distribution for each class adding up to 1 for output layer\n",
        "    x = F.softmax(self.fc4(x), dim=1)\n",
        "    return x\n",
        "\n",
        "#declare a model\n",
        "net = Net()\n",
        "#print(net)\n",
        "#passing in random data\n",
        "x = torch.rand(28,28)\n",
        "#resize to represent input shape (batch size, input x, input y)\n",
        "x = x.view(-1,28,28)\n",
        "#print(x)\n",
        "\n",
        "#optmizer adjusts neural network based on error calculation\n",
        "import torch.optim as optim \n",
        "#net.parameters() means all the adjustable parts of the neural network, learning rate is amount of change (we don't want model to swerve based on one train case)\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
        "\n",
        "#full pass through data is epoch\n",
        "#activates dropout and batch norm\n",
        "net.train()\n",
        "EPOCHS = 3\n",
        "for epoch in range(EPOCHS):\n",
        "  #data is a batch of data in the training set\n",
        "  for data in trainset:\n",
        "    #split into features and labels\n",
        "    features, labels = data\n",
        "\n",
        "    #reset the gradient for next passes to avoid convoluting the results of multiple backpropogations \n",
        "    #(gradients contains the loss with respect to trainable parameters)\n",
        "    net.zero_grad()\n",
        "    #pass data into network (make sure input shape matches)\n",
        "    output = net(features.view(-1,28*28))\n",
        "    #compute error (output,expected)\n",
        "    loss = F.nll_loss(output,labels)\n",
        "    #backpropogate loss through trainiable parameters of model and calculate gradients \n",
        "    loss.backward()\n",
        "    #adjust neural network\n",
        "    optimizer.step()\n",
        "  print(\"loss is \", loss)\n",
        "\n",
        "#evaluate accuracy of model\n",
        "correct = 0\n",
        "total = 0\n",
        "#deactivates batch norm and dropout\n",
        "net.eval()\n",
        "#deactivates calculating gradients \n",
        "with torch.no_grad():\n",
        "  for data in trainset:\n",
        "    images, labels = data\n",
        "    output = net(images.view(-1,784))\n",
        "    correct += output.argmax(dim=1).eq(labels).sum().item()\n",
        "    total += len(labels)\n",
        "print(\"Accuracy \", round(correct/total,3))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.8829, grad_fn=<NllLossBackward>)\n",
            "tensor(-0.9000, grad_fn=<NllLossBackward>)\n",
            "tensor(-1., grad_fn=<NllLossBackward>)\n",
            "Accuracy  0.946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Qnpdn3iQfJci",
        "outputId": "918f05f2-6f7a-4c77-a08e-dde26995c007"
      },
      "source": [
        "#visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(images[1].view(28,28))\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}